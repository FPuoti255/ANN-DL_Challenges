{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Artifical Neural Networks & Deep Learning\n# Homework 1 - Image Classification\n\n**Developement Team:**\n- Acquati Marco - 10583134 \n- Brugali Giorgio - 10794550\n- Puoti Francesco - 10595640 \n\n\n# *1. Data acquisition and augmentation*\n\n> This topic has been issued with annotations as the code flows down, to better clarify the correspondence between the explanations and the code snippets \n\n\n# *2. Model overview*\n\n\n> ***2.1. Features' Extraction***\n\n>> The feature extractor is composed by *5 blocks*, each composed by:\n  - one convolutional part\n  - one  MaxPooling layer.\n  \n>>The convolutional part has a variable number of convolutional layers: 2 Conv2D layers for the first two blocks and 3 Conv2D layers for the remaining ones.\nThis choice was dictated with the intention of reducing the parameters' number in the network.Our idea was to start with small filters 3x3 because of the need to detail the features' extraction at the beginning. Afterwards, instead of increasing the filters' size, to avoid having possible distorted features that could result too effective in the learning process, we decided to augment the number of convolutional layers.\nRegarding the activation function, we chose the ReLU activation function as it is more suitable for the classification problem.\n\n>>BatchNormalization has been involved in the features' extraction part in order to both improve the stability of our network and to reduce covariance shift, the latter resulting in improving the training velocity as well. \n\n>>The pool size has been set to 3x3 with stride 2x2 to favor the overlapping: it has been demonstrated that the overlapping pooling areas reduce the likelihood of the network to overfit. \n\n> ***2.2. BottleNeck Layer***\n>> In order to reduce the computational load of the network, \nthe number of the features extractorâ€™s output channels is reduced by adding a 1x1 convolutional layer before feeding the output to the classifier.\n\n> ***2.3. Classifier***\n>> - one flatten layer\n>> - two dense hidden layers with 2048 neurons each\n>> - the output layer with the SOFTMAX activation function and three classes\n>> It's worth highlighting the use of weight initialization (HeNormal distribution), which aims at improving the network speed, avoiding too many zeroes in the kernels at the beginning of the learning process.\n>> Moreover, weight decay has been implemented to reduce overfitting in the Dense layers.\n>> Both BatchNormalization and ReLU have been used for the same purpose as in the features' extraction part.\n\n> ***2.4. Optimizer & LossFunction***\n>> - Adam, with a starting learning rate of 1e-3 and amsgrad = True to have an adaptive learning rate, so as to prevent the network from being stuck on a suboptimal solution.\n>> - Loss function : Categorical Crossentropy.\n\n> ***2.5. Further information about the implemention process***\n>> No EarlyStopping has been used in the final model as, after some trials, such model got stopped even though the learning process would have subsequently led to noteworthy improvements.\n>> Model checkpoints could not be implemented due to memory shortage.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\nSEED = 1234\ntf.random.set_seed(SEED)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport json\nimport operator\n\n# Since the training images were not divided in subfolders, \n#     we had to manage the data acquisition by means of data frame. \n# First, we acquired the images' paths an we sorted them in order to create \n#     a correspondence between each image and its label stored in the json file.\n#-------------------------------------------------------------------------------\n\nX = [] #list of images' paths\nfor dirname, _, filenames in os.walk('../input/artificial-neural-networks-and-deep-learning-2020/MaskDataset/training'):\n    filenames.sort()\n    for filename in filenames:\n           X.append(os.path.join(dirname, filename))\n\nwith open('/kaggle/input/artificial-neural-networks-and-deep-learning-2020/MaskDataset/train_gt.json') as f:\n\n data = json.load(f)\n\ndata = sorted(data.items(), key=operator.itemgetter(0))\n\ny = [] #list of target labels\nfor i in range(len(data)):\n    y.append(str(data[i][1])) #il dataframe vuole delle string","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# We decided to apply data augmentation on the training set not because of the data shortage\n# but in order to make our model more flexibile on recognizing objects in different positions and dimensions.\n#------------------------------------------------------------------------------------------------------------\n\ntrain_data_gen = ImageDataGenerator(rotation_range=10,\n                                    width_shift_range=10,\n                                    height_shift_range=10,\n                                    zoom_range=0.3,\n                                    horizontal_flip=True,\n                                    vertical_flip=True,\n                                    fill_mode='nearest',\n                                    rescale=1./255)\n\n\n# No data aumentation has been applied on validation set, since we want to have the images meant for validation \n# similar to the test images to find out the features of our model\n#--------------------------------------------------------------------------------------------------------------\nvalid_data_gen = ImageDataGenerator(rescale=1./255)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Data set split in training and validation sets \n# with the latter having a size equal to the 20% of the entire data set.\n#-----------------------------------------------------------------------\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)\n\n\n# Generation of training and validation dataframes using pandas' library\n#------------------------------------------------------------------------\ndataframe_train = pd.DataFrame({\"input\": X_train, \"target\": y_train})\ndataframe_valid = pd.DataFrame({\"input\": X_valid, \"target\": y_valid})\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size\nbs = 32\n\n# img shape\nimg_h = 256\nimg_w = 256\n\nnum_classes = 3\n\nclasses = ['0', '1', '2']\nclmode = \"rgb\"\n\n# Creation of the DataFrameIterators yielding tuples of (x, y) \n# where x is a numpy array containing a batch of images with shape (batch_size, *target_size, channels) \n# and y is a numpy array of corresponding labels.\n#------------------------------------------------------------------------------------------------------\n\ntrain_datagen = train_data_gen.flow_from_dataframe(\n      dataframe = dataframe_train,\n      directory = './',\n      x_col = \"input\",\n      y_col = \"target\",\n      target_size = (img_h, img_w),\n      color_mode = clmode,\n      classes = classes,\n      class_mode = \"categorical\",\n      batch_size = bs,\n      shuffle = True,\n      seed = SEED\n)\n\nvalid_datagen = valid_data_gen.flow_from_dataframe(\n      dataframe = dataframe_valid,\n      directory = './',\n      x_col = \"input\",\n      y_col = \"target\",\n      target_size = (img_h, img_w),\n      color_mode = clmode,\n      classes = classes,\n      class_mode = \"categorical\",\n      batch_size = bs,\n      shuffle = True,\n      seed = SEED\n)","execution_count":5,"outputs":[{"output_type":"stream","text":"Found 4210 validated image filenames belonging to 3 classes.\nFound 1404 validated image filenames belonging to 3 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dataset objects\n# ----------------------\n\n# Training\ntrain_dataset = tf.data.Dataset.from_generator(lambda: train_datagen,\n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes])) #None for dynamic bs, \ntrain_dataset = train_dataset.repeat()\n\n# Validation\n# ----------\nvalid_dataset = tf.data.Dataset.from_generator(lambda: valid_datagen, \n                                               output_types=(tf.float32, tf.float32),\n                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\nvalid_dataset = valid_dataset.repeat()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------CALLBACKS' SELECTION--------\n#--------------------------------------\n\nfrom datetime import datetime\ncallbacks = []\nnow = datetime.now().strftime('%b%d_%H-%M-%S')\n\nexp_dir = './logs'\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n\nexp_dir = os.path.join(exp_dir, '_' + str(now))\nif not os.path.exists(exp_dir):\n    os.makedirs(exp_dir)\n\n# Model checkpoint\n# ----------------\nckpt_dir = os.path.join(exp_dir, 'checkpoint')\nif not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)    \nckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=(ckpt_dir + '/model_' + 'cp_{epoch:02d}.h5'), \n                                            monitor='val_accuracy', verbose=1,\n                                            save_best_only=True, mode='max') \ncallbacks.append(ckpt_callback)\n\n\n# Early Stopping\n# --------------\nes = False\nif es:\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n    callbacks.append(es_callback)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\n    \nmodel = tf.keras.Sequential()\n\ndepth = 5\nstart_f = 64\nker_sz = (3,3)\nker_str = (1,1)\npool_sz = (3,3)\n\ninitializer = tf.keras.initializers.he_normal(seed=SEED)\nregularizer = tf.keras.regularizers.l2(0.001) #weight decay\ndpout = 0.5\n\nconvNumb = 2\n\n# Features extraction\n# -------------------\nfor i in range(depth):\n\n    if i == 0:\n        input_shape = [img_h, img_w, 3]\n    else:\n        input_shape=[None]\n        \n    if i == 2:\n        convNumb += 1 #increasing number of Conv2D layer in a block\n        \n    for j in range(convNumb):\n        model.add(tf.keras.layers.Conv2D(filters=start_f, \n                                 kernel_size=ker_sz,\n                                 strides=ker_str,\n                                 padding='same',\n                                 input_shape=input_shape))        \n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.ReLU())\n    model.add(tf.keras.layers.MaxPool2D(pool_sz, strides = 2))\n    if start_f < 512:\n        start_f *= 2\n\n#Bottle neck layer\n#-----------------\nmodel.add(tf.keras.layers.Conv2D(filters=start_f/2, \n                              kernel_size=(1,1),\n                              strides=ker_str,\n                              padding='same',\n                              input_shape=input_shape))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.ReLU())     \n\n#Classifier\n#----------\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(units=2048, \n                                kernel_regularizer = regularizer,\n                                kernel_initializer = initializer))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.ReLU()) \nmodel.add(tf.keras.layers.Dropout(dpout))\nmodel.add(tf.keras.layers.Dense(units=2048,\n                                kernel_regularizer=regularizer, \n                                kernel_initializer = initializer))\nmodel.add(tf.keras.layers.ReLU())\nmodel.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, amsgrad=True), \n              loss=tf.keras.losses.CategoricalCrossentropy(), \n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":20,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_14 (Conv2D)           (None, 256, 256, 64)      1792      \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 256, 256, 64)      256       \n_________________________________________________________________\nre_lu_16 (ReLU)              (None, 256, 256, 64)      0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 256, 256, 64)      36928     \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 256, 256, 64)      256       \n_________________________________________________________________\nre_lu_17 (ReLU)              (None, 256, 256, 64)      0         \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 127, 127, 64)      0         \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 127, 127, 128)     73856     \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 127, 127, 128)     512       \n_________________________________________________________________\nre_lu_18 (ReLU)              (None, 127, 127, 128)     0         \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 127, 127, 128)     147584    \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 127, 127, 128)     512       \n_________________________________________________________________\nre_lu_19 (ReLU)              (None, 127, 127, 128)     0         \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 63, 63, 128)       0         \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 63, 63, 256)       295168    \n_________________________________________________________________\nbatch_normalization_19 (Batc (None, 63, 63, 256)       1024      \n_________________________________________________________________\nre_lu_20 (ReLU)              (None, 63, 63, 256)       0         \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 63, 63, 256)       590080    \n_________________________________________________________________\nbatch_normalization_20 (Batc (None, 63, 63, 256)       1024      \n_________________________________________________________________\nre_lu_21 (ReLU)              (None, 63, 63, 256)       0         \n_________________________________________________________________\nconv2d_20 (Conv2D)           (None, 63, 63, 256)       590080    \n_________________________________________________________________\nbatch_normalization_21 (Batc (None, 63, 63, 256)       1024      \n_________________________________________________________________\nre_lu_22 (ReLU)              (None, 63, 63, 256)       0         \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 31, 31, 256)       0         \n_________________________________________________________________\nconv2d_21 (Conv2D)           (None, 31, 31, 512)       1180160   \n_________________________________________________________________\nbatch_normalization_22 (Batc (None, 31, 31, 512)       2048      \n_________________________________________________________________\nre_lu_23 (ReLU)              (None, 31, 31, 512)       0         \n_________________________________________________________________\nconv2d_22 (Conv2D)           (None, 31, 31, 512)       2359808   \n_________________________________________________________________\nbatch_normalization_23 (Batc (None, 31, 31, 512)       2048      \n_________________________________________________________________\nre_lu_24 (ReLU)              (None, 31, 31, 512)       0         \n_________________________________________________________________\nconv2d_23 (Conv2D)           (None, 31, 31, 512)       2359808   \n_________________________________________________________________\nbatch_normalization_24 (Batc (None, 31, 31, 512)       2048      \n_________________________________________________________________\nre_lu_25 (ReLU)              (None, 31, 31, 512)       0         \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 15, 15, 512)       0         \n_________________________________________________________________\nconv2d_24 (Conv2D)           (None, 15, 15, 512)       2359808   \n_________________________________________________________________\nbatch_normalization_25 (Batc (None, 15, 15, 512)       2048      \n_________________________________________________________________\nre_lu_26 (ReLU)              (None, 15, 15, 512)       0         \n_________________________________________________________________\nconv2d_25 (Conv2D)           (None, 15, 15, 512)       2359808   \n_________________________________________________________________\nbatch_normalization_26 (Batc (None, 15, 15, 512)       2048      \n_________________________________________________________________\nre_lu_27 (ReLU)              (None, 15, 15, 512)       0         \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 15, 15, 512)       2359808   \n_________________________________________________________________\nbatch_normalization_27 (Batc (None, 15, 15, 512)       2048      \n_________________________________________________________________\nre_lu_28 (ReLU)              (None, 15, 15, 512)       0         \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 7, 7, 512)         0         \n_________________________________________________________________\nconv2d_27 (Conv2D)           (None, 7, 7, 256)         131328    \n_________________________________________________________________\nbatch_normalization_28 (Batc (None, 7, 7, 256)         1024      \n_________________________________________________________________\nre_lu_29 (ReLU)              (None, 7, 7, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 12544)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2048)              25692160  \n_________________________________________________________________\nbatch_normalization_29 (Batc (None, 2048)              8192      \n_________________________________________________________________\nre_lu_30 (ReLU)              (None, 2048)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 2048)              4196352   \n_________________________________________________________________\nre_lu_31 (ReLU)              (None, 2048)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 3)                 6147      \n=================================================================\nTotal params: 44,766,787\nTrainable params: 44,753,731\nNon-trainable params: 13,056\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=train_dataset,\n          epochs=200,\n          steps_per_epoch=len(train_datagen),\n          validation_data=valid_dataset,\n          validation_steps=len(valid_datagen),\n          callbacks=callbacks\n         )","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/200\n132/132 [==============================] - ETA: 0s - loss: 8.9970 - accuracy: 0.3444\nEpoch 00001: val_accuracy improved from -inf to 0.33120, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_01.h5\n132/132 [==============================] - 144s 1s/step - loss: 8.9970 - accuracy: 0.3444 - val_loss: 6.6715 - val_accuracy: 0.3312\nEpoch 2/200\n132/132 [==============================] - ETA: 0s - loss: 5.2488 - accuracy: 0.3366\nEpoch 00002: val_accuracy improved from 0.33120 to 0.34544, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_02.h5\n132/132 [==============================] - 143s 1s/step - loss: 5.2488 - accuracy: 0.3366 - val_loss: 4.0577 - val_accuracy: 0.3454\nEpoch 3/200\n132/132 [==============================] - ETA: 0s - loss: 3.3922 - accuracy: 0.3347\nEpoch 00003: val_accuracy did not improve from 0.34544\n132/132 [==============================] - 142s 1s/step - loss: 3.3922 - accuracy: 0.3347 - val_loss: 2.8104 - val_accuracy: 0.3340\nEpoch 4/200\n132/132 [==============================] - ETA: 0s - loss: 2.4703 - accuracy: 0.3482\nEpoch 00004: val_accuracy did not improve from 0.34544\n132/132 [==============================] - 142s 1s/step - loss: 2.4703 - accuracy: 0.3482 - val_loss: 2.1550 - val_accuracy: 0.3376\nEpoch 5/200\n132/132 [==============================] - ETA: 0s - loss: 1.9889 - accuracy: 0.3601\nEpoch 00005: val_accuracy improved from 0.34544 to 0.35256, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_05.h5\n132/132 [==============================] - 143s 1s/step - loss: 1.9889 - accuracy: 0.3601 - val_loss: 1.8249 - val_accuracy: 0.3526\nEpoch 6/200\n132/132 [==============================] - ETA: 0s - loss: 1.6564 - accuracy: 0.4378\nEpoch 00006: val_accuracy did not improve from 0.35256\n132/132 [==============================] - 139s 1s/step - loss: 1.6564 - accuracy: 0.4378 - val_loss: 5.0574 - val_accuracy: 0.3291\nEpoch 7/200\n132/132 [==============================] - ETA: 0s - loss: 1.3899 - accuracy: 0.5354\nEpoch 00007: val_accuracy did not improve from 0.35256\n132/132 [==============================] - 140s 1s/step - loss: 1.3899 - accuracy: 0.5354 - val_loss: 3.5958 - val_accuracy: 0.3348\nEpoch 8/200\n132/132 [==============================] - ETA: 0s - loss: 1.1708 - accuracy: 0.5917\nEpoch 00008: val_accuracy improved from 0.35256 to 0.65242, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_08.h5\n132/132 [==============================] - 141s 1s/step - loss: 1.1708 - accuracy: 0.5917 - val_loss: 1.0024 - val_accuracy: 0.6524\nEpoch 9/200\n132/132 [==============================] - ETA: 0s - loss: 0.9794 - accuracy: 0.6570\nEpoch 00009: val_accuracy did not improve from 0.65242\n132/132 [==============================] - 139s 1s/step - loss: 0.9794 - accuracy: 0.6570 - val_loss: 1.2120 - val_accuracy: 0.4872\nEpoch 10/200\n132/132 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.6686\nEpoch 00010: val_accuracy improved from 0.65242 to 0.66239, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_10.h5\n132/132 [==============================] - 146s 1s/step - loss: 0.9142 - accuracy: 0.6686 - val_loss: 0.9275 - val_accuracy: 0.6624\nEpoch 11/200\n132/132 [==============================] - ETA: 0s - loss: 0.8604 - accuracy: 0.6917\nEpoch 00011: val_accuracy did not improve from 0.66239\n132/132 [==============================] - 144s 1s/step - loss: 0.8604 - accuracy: 0.6917 - val_loss: 1.1345 - val_accuracy: 0.5983\nEpoch 12/200\n132/132 [==============================] - ETA: 0s - loss: 0.7749 - accuracy: 0.7162\nEpoch 00012: val_accuracy improved from 0.66239 to 0.68091, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_12.h5\n132/132 [==============================] - 149s 1s/step - loss: 0.7749 - accuracy: 0.7162 - val_loss: 0.8243 - val_accuracy: 0.6809\nEpoch 13/200\n132/132 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.7333\nEpoch 00013: val_accuracy improved from 0.68091 to 0.72151, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_13.h5\n132/132 [==============================] - 148s 1s/step - loss: 0.7313 - accuracy: 0.7333 - val_loss: 0.7585 - val_accuracy: 0.7215\nEpoch 14/200\n132/132 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.7534\nEpoch 00014: val_accuracy did not improve from 0.72151\n132/132 [==============================] - 148s 1s/step - loss: 0.6953 - accuracy: 0.7534 - val_loss: 0.9877 - val_accuracy: 0.6766\nEpoch 15/200\n132/132 [==============================] - ETA: 0s - loss: 0.6478 - accuracy: 0.7703\nEpoch 00015: val_accuracy did not improve from 0.72151\n132/132 [==============================] - 146s 1s/step - loss: 0.6478 - accuracy: 0.7703 - val_loss: 0.8547 - val_accuracy: 0.6838\nEpoch 16/200\n132/132 [==============================] - ETA: 0s - loss: 0.6438 - accuracy: 0.7679\nEpoch 00016: val_accuracy improved from 0.72151 to 0.74359, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_16.h5\n132/132 [==============================] - 146s 1s/step - loss: 0.6438 - accuracy: 0.7679 - val_loss: 0.7888 - val_accuracy: 0.7436\nEpoch 17/200\n132/132 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.7758\nEpoch 00017: val_accuracy did not improve from 0.74359\n132/132 [==============================] - 147s 1s/step - loss: 0.6142 - accuracy: 0.7758 - val_loss: 0.9458 - val_accuracy: 0.7051\nEpoch 18/200\n132/132 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.8026\nEpoch 00018: val_accuracy did not improve from 0.74359\n132/132 [==============================] - 145s 1s/step - loss: 0.5803 - accuracy: 0.8026 - val_loss: 0.7369 - val_accuracy: 0.6944\nEpoch 19/200\n132/132 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7990\nEpoch 00019: val_accuracy improved from 0.74359 to 0.78134, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_19.h5\n132/132 [==============================] - 147s 1s/step - loss: 0.5773 - accuracy: 0.7990 - val_loss: 0.6038 - val_accuracy: 0.7813\nEpoch 20/200\n132/132 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.7907\nEpoch 00020: val_accuracy did not improve from 0.78134\n132/132 [==============================] - 145s 1s/step - loss: 0.5804 - accuracy: 0.7907 - val_loss: 0.7599 - val_accuracy: 0.7009\nEpoch 21/200\n132/132 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.8119\nEpoch 00021: val_accuracy did not improve from 0.78134\n132/132 [==============================] - 146s 1s/step - loss: 0.5375 - accuracy: 0.8119 - val_loss: 0.6870 - val_accuracy: 0.7557\nEpoch 22/200\n132/132 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.8069\nEpoch 00022: val_accuracy did not improve from 0.78134\n132/132 [==============================] - 145s 1s/step - loss: 0.5490 - accuracy: 0.8069 - val_loss: 0.7925 - val_accuracy: 0.6973\nEpoch 23/200\n132/132 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.8112\nEpoch 00023: val_accuracy did not improve from 0.78134\n132/132 [==============================] - 145s 1s/step - loss: 0.5283 - accuracy: 0.8112 - val_loss: 0.6073 - val_accuracy: 0.7735\nEpoch 24/200\n132/132 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.8173\nEpoch 00024: val_accuracy improved from 0.78134 to 0.80413, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_24.h5\n132/132 [==============================] - 148s 1s/step - loss: 0.5322 - accuracy: 0.8173 - val_loss: 0.5089 - val_accuracy: 0.8041\nEpoch 25/200\n132/132 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.8299\nEpoch 00025: val_accuracy did not improve from 0.80413\n132/132 [==============================] - 149s 1s/step - loss: 0.4998 - accuracy: 0.8299 - val_loss: 0.8276 - val_accuracy: 0.7479\nEpoch 26/200\n132/132 [==============================] - ETA: 0s - loss: 0.5301 - accuracy: 0.8169\nEpoch 00026: val_accuracy improved from 0.80413 to 0.81695, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_26.h5\n132/132 [==============================] - 150s 1s/step - loss: 0.5301 - accuracy: 0.8169 - val_loss: 0.5193 - val_accuracy: 0.8170\nEpoch 27/200\n","name":"stdout"},{"output_type":"stream","text":"132/132 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.8333\nEpoch 00027: val_accuracy did not improve from 0.81695\n132/132 [==============================] - 150s 1s/step - loss: 0.4974 - accuracy: 0.8333 - val_loss: 0.8698 - val_accuracy: 0.6688\nEpoch 28/200\n132/132 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.8302\nEpoch 00028: val_accuracy did not improve from 0.81695\n132/132 [==============================] - 147s 1s/step - loss: 0.4928 - accuracy: 0.8302 - val_loss: 0.6392 - val_accuracy: 0.7671\nEpoch 29/200\n132/132 [==============================] - ETA: 0s - loss: 0.5191 - accuracy: 0.8273\nEpoch 00029: val_accuracy improved from 0.81695 to 0.82906, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_29.h5\n132/132 [==============================] - 151s 1s/step - loss: 0.5191 - accuracy: 0.8273 - val_loss: 0.5069 - val_accuracy: 0.8291\nEpoch 30/200\n132/132 [==============================] - ETA: 0s - loss: 0.4638 - accuracy: 0.8466\nEpoch 00030: val_accuracy did not improve from 0.82906\n132/132 [==============================] - 149s 1s/step - loss: 0.4638 - accuracy: 0.8466 - val_loss: 0.9237 - val_accuracy: 0.7130\nEpoch 31/200\n132/132 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.8363\nEpoch 00031: val_accuracy did not improve from 0.82906\n132/132 [==============================] - 147s 1s/step - loss: 0.5022 - accuracy: 0.8363 - val_loss: 0.6499 - val_accuracy: 0.7699\nEpoch 32/200\n132/132 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.8354\nEpoch 00032: val_accuracy improved from 0.82906 to 0.83048, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_32.h5\n132/132 [==============================] - 149s 1s/step - loss: 0.4798 - accuracy: 0.8354 - val_loss: 0.4579 - val_accuracy: 0.8305\nEpoch 33/200\n132/132 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.8375\nEpoch 00033: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 150s 1s/step - loss: 0.4709 - accuracy: 0.8375 - val_loss: 0.5679 - val_accuracy: 0.7849\nEpoch 34/200\n132/132 [==============================] - ETA: 0s - loss: 0.4663 - accuracy: 0.8463\nEpoch 00034: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 148s 1s/step - loss: 0.4663 - accuracy: 0.8463 - val_loss: 0.9618 - val_accuracy: 0.6538\nEpoch 35/200\n132/132 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.8482\nEpoch 00035: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 146s 1s/step - loss: 0.4558 - accuracy: 0.8482 - val_loss: 0.4954 - val_accuracy: 0.8148\nEpoch 36/200\n132/132 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.8466\nEpoch 00036: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 145s 1s/step - loss: 0.4554 - accuracy: 0.8466 - val_loss: 0.4971 - val_accuracy: 0.8248\nEpoch 37/200\n132/132 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8506\nEpoch 00037: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 147s 1s/step - loss: 0.4407 - accuracy: 0.8506 - val_loss: 0.6304 - val_accuracy: 0.7828\nEpoch 38/200\n132/132 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8542\nEpoch 00038: val_accuracy did not improve from 0.83048\n132/132 [==============================] - 147s 1s/step - loss: 0.4391 - accuracy: 0.8542 - val_loss: 0.6850 - val_accuracy: 0.7443\nEpoch 39/200\n132/132 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8513\nEpoch 00039: val_accuracy improved from 0.83048 to 0.84687, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_39.h5\n132/132 [==============================] - 148s 1s/step - loss: 0.4303 - accuracy: 0.8513 - val_loss: 0.4774 - val_accuracy: 0.8469\nEpoch 40/200\n132/132 [==============================] - ETA: 0s - loss: 0.4228 - accuracy: 0.8568\nEpoch 00040: val_accuracy did not improve from 0.84687\n132/132 [==============================] - 150s 1s/step - loss: 0.4228 - accuracy: 0.8568 - val_loss: 0.5064 - val_accuracy: 0.8234\nEpoch 41/200\n132/132 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8587\nEpoch 00041: val_accuracy did not improve from 0.84687\n132/132 [==============================] - 146s 1s/step - loss: 0.4479 - accuracy: 0.8587 - val_loss: 0.4713 - val_accuracy: 0.8376\nEpoch 42/200\n132/132 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8660\nEpoch 00042: val_accuracy did not improve from 0.84687\n132/132 [==============================] - 147s 1s/step - loss: 0.4036 - accuracy: 0.8660 - val_loss: 0.5734 - val_accuracy: 0.7657\nEpoch 43/200\n132/132 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8684\nEpoch 00043: val_accuracy did not improve from 0.84687\n132/132 [==============================] - 146s 1s/step - loss: 0.4117 - accuracy: 0.8684 - val_loss: 0.4693 - val_accuracy: 0.8376\nEpoch 44/200\n132/132 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.8587\nEpoch 00044: val_accuracy improved from 0.84687 to 0.87749, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_44.h5\n132/132 [==============================] - 145s 1s/step - loss: 0.4259 - accuracy: 0.8587 - val_loss: 0.4138 - val_accuracy: 0.8775\nEpoch 45/200\n132/132 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8703\nEpoch 00045: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 148s 1s/step - loss: 0.4045 - accuracy: 0.8703 - val_loss: 0.7142 - val_accuracy: 0.7258\nEpoch 46/200\n132/132 [==============================] - ETA: 0s - loss: 0.4101 - accuracy: 0.8691\nEpoch 00046: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 148s 1s/step - loss: 0.4101 - accuracy: 0.8691 - val_loss: 0.4037 - val_accuracy: 0.8711\nEpoch 47/200\n132/132 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8746\nEpoch 00047: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 145s 1s/step - loss: 0.3912 - accuracy: 0.8746 - val_loss: 0.3998 - val_accuracy: 0.8697\nEpoch 48/200\n132/132 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.8658\nEpoch 00048: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 145s 1s/step - loss: 0.4144 - accuracy: 0.8658 - val_loss: 0.5507 - val_accuracy: 0.7942\nEpoch 49/200\n132/132 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8765\nEpoch 00049: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 146s 1s/step - loss: 0.3997 - accuracy: 0.8765 - val_loss: 0.9397 - val_accuracy: 0.7051\nEpoch 50/200\n132/132 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.8739\nEpoch 00050: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 141s 1s/step - loss: 0.3971 - accuracy: 0.8739 - val_loss: 0.5383 - val_accuracy: 0.8162\nEpoch 51/200\n132/132 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8793\nEpoch 00051: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 142s 1s/step - loss: 0.3894 - accuracy: 0.8793 - val_loss: 0.3779 - val_accuracy: 0.8697\nEpoch 52/200\n132/132 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.8717\nEpoch 00052: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 136s 1s/step - loss: 0.3958 - accuracy: 0.8717 - val_loss: 0.4721 - val_accuracy: 0.8490\nEpoch 53/200\n132/132 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.8862\nEpoch 00053: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 138s 1s/step - loss: 0.3647 - accuracy: 0.8862 - val_loss: 0.4354 - val_accuracy: 0.8426\nEpoch 54/200\n132/132 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.8800\nEpoch 00054: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 138s 1s/step - loss: 0.4042 - accuracy: 0.8800 - val_loss: 0.4287 - val_accuracy: 0.8497\n","name":"stdout"},{"output_type":"stream","text":"Epoch 55/200\n132/132 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8765\nEpoch 00055: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 137s 1s/step - loss: 0.3968 - accuracy: 0.8765 - val_loss: 0.4589 - val_accuracy: 0.8340\nEpoch 56/200\n132/132 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8914\nEpoch 00056: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 139s 1s/step - loss: 0.3466 - accuracy: 0.8914 - val_loss: 0.5370 - val_accuracy: 0.8170\nEpoch 57/200\n132/132 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.8793\nEpoch 00057: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 140s 1s/step - loss: 0.3852 - accuracy: 0.8793 - val_loss: 0.5057 - val_accuracy: 0.8212\nEpoch 58/200\n132/132 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8874\nEpoch 00058: val_accuracy did not improve from 0.87749\n132/132 [==============================] - 141s 1s/step - loss: 0.3761 - accuracy: 0.8874 - val_loss: 0.4944 - val_accuracy: 0.8177\nEpoch 59/200\n132/132 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.8931\nEpoch 00059: val_accuracy improved from 0.87749 to 0.88177, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_59.h5\n132/132 [==============================] - 140s 1s/step - loss: 0.3649 - accuracy: 0.8931 - val_loss: 0.3668 - val_accuracy: 0.8818\nEpoch 60/200\n132/132 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8919\nEpoch 00060: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 139s 1s/step - loss: 0.3539 - accuracy: 0.8919 - val_loss: 0.3704 - val_accuracy: 0.8789\nEpoch 61/200\n132/132 [==============================] - ETA: 0s - loss: 0.3540 - accuracy: 0.8931\nEpoch 00061: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 139s 1s/step - loss: 0.3540 - accuracy: 0.8931 - val_loss: 0.6869 - val_accuracy: 0.7735\nEpoch 62/200\n132/132 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8914\nEpoch 00062: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 139s 1s/step - loss: 0.3631 - accuracy: 0.8914 - val_loss: 0.3986 - val_accuracy: 0.8711\nEpoch 63/200\n132/132 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8976\nEpoch 00063: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 140s 1s/step - loss: 0.3398 - accuracy: 0.8976 - val_loss: 0.4013 - val_accuracy: 0.8583\nEpoch 64/200\n132/132 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9019\nEpoch 00064: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 138s 1s/step - loss: 0.3375 - accuracy: 0.9019 - val_loss: 0.5871 - val_accuracy: 0.8041\nEpoch 65/200\n132/132 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8857\nEpoch 00065: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 138s 1s/step - loss: 0.3648 - accuracy: 0.8857 - val_loss: 0.3798 - val_accuracy: 0.8789\nEpoch 66/200\n132/132 [==============================] - ETA: 0s - loss: 0.3475 - accuracy: 0.8964\nEpoch 00066: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 135s 1s/step - loss: 0.3475 - accuracy: 0.8964 - val_loss: 0.6026 - val_accuracy: 0.8333\nEpoch 67/200\n132/132 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.9024\nEpoch 00067: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 139s 1s/step - loss: 0.3437 - accuracy: 0.9024 - val_loss: 0.5009 - val_accuracy: 0.8105\nEpoch 68/200\n132/132 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.8974\nEpoch 00068: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 137s 1s/step - loss: 0.3409 - accuracy: 0.8974 - val_loss: 0.3912 - val_accuracy: 0.8761\nEpoch 69/200\n132/132 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9067\nEpoch 00069: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 139s 1s/step - loss: 0.3348 - accuracy: 0.9067 - val_loss: 0.4468 - val_accuracy: 0.8440\nEpoch 70/200\n132/132 [==============================] - ETA: 0s - loss: 0.3300 - accuracy: 0.9040\nEpoch 00070: val_accuracy did not improve from 0.88177\n132/132 [==============================] - 135s 1s/step - loss: 0.3300 - accuracy: 0.9040 - val_loss: 0.3843 - val_accuracy: 0.8768\nEpoch 71/200\n132/132 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9057\nEpoch 00071: val_accuracy improved from 0.88177 to 0.88818, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_71.h5\n132/132 [==============================] - 138s 1s/step - loss: 0.3185 - accuracy: 0.9057 - val_loss: 0.3639 - val_accuracy: 0.8882\nEpoch 72/200\n132/132 [==============================] - ETA: 0s - loss: 0.3205 - accuracy: 0.9059\nEpoch 00072: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 138s 1s/step - loss: 0.3205 - accuracy: 0.9059 - val_loss: 0.4745 - val_accuracy: 0.8554\nEpoch 73/200\n132/132 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9040\nEpoch 00073: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 145s 1s/step - loss: 0.3383 - accuracy: 0.9040 - val_loss: 0.6166 - val_accuracy: 0.7942\nEpoch 74/200\n132/132 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8993\nEpoch 00074: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.3499 - accuracy: 0.8993 - val_loss: 0.5572 - val_accuracy: 0.8291\nEpoch 75/200\n132/132 [==============================] - ETA: 0s - loss: 0.3367 - accuracy: 0.9021\nEpoch 00075: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.3367 - accuracy: 0.9021 - val_loss: 0.6082 - val_accuracy: 0.8383\nEpoch 76/200\n132/132 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.9076\nEpoch 00076: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.3342 - accuracy: 0.9076 - val_loss: 0.5163 - val_accuracy: 0.8326\nEpoch 77/200\n132/132 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9050\nEpoch 00077: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 138s 1s/step - loss: 0.3341 - accuracy: 0.9050 - val_loss: 0.6730 - val_accuracy: 0.7635\nEpoch 78/200\n132/132 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.9088\nEpoch 00078: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.3164 - accuracy: 0.9088 - val_loss: 0.4116 - val_accuracy: 0.8654\nEpoch 79/200\n132/132 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.9074\nEpoch 00079: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.3358 - accuracy: 0.9074 - val_loss: 0.5577 - val_accuracy: 0.8355\nEpoch 80/200\n132/132 [==============================] - ETA: 0s - loss: 0.3263 - accuracy: 0.9040\nEpoch 00080: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 135s 1s/step - loss: 0.3263 - accuracy: 0.9040 - val_loss: 0.3777 - val_accuracy: 0.8839\nEpoch 81/200\n132/132 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.9162\nEpoch 00081: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 133s 1s/step - loss: 0.3046 - accuracy: 0.9162 - val_loss: 0.6826 - val_accuracy: 0.7877\nEpoch 82/200\n132/132 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.9055\nEpoch 00082: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 132s 1s/step - loss: 0.3193 - accuracy: 0.9055 - val_loss: 0.6599 - val_accuracy: 0.8077\nEpoch 83/200\n","name":"stdout"},{"output_type":"stream","text":"132/132 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.9107\nEpoch 00083: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 132s 999ms/step - loss: 0.3179 - accuracy: 0.9107 - val_loss: 0.4551 - val_accuracy: 0.8447\nEpoch 84/200\n132/132 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9088\nEpoch 00084: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 132s 999ms/step - loss: 0.2942 - accuracy: 0.9088 - val_loss: 0.5462 - val_accuracy: 0.8469\nEpoch 85/200\n132/132 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9133\nEpoch 00085: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 136s 1s/step - loss: 0.3110 - accuracy: 0.9133 - val_loss: 0.4943 - val_accuracy: 0.8255\nEpoch 86/200\n132/132 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9178\nEpoch 00086: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 133s 1s/step - loss: 0.2948 - accuracy: 0.9178 - val_loss: 0.3399 - val_accuracy: 0.8875\nEpoch 87/200\n132/132 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9195\nEpoch 00087: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 131s 994ms/step - loss: 0.2923 - accuracy: 0.9195 - val_loss: 0.4423 - val_accuracy: 0.8704\nEpoch 88/200\n132/132 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9166\nEpoch 00088: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 132s 997ms/step - loss: 0.2905 - accuracy: 0.9166 - val_loss: 0.4596 - val_accuracy: 0.8497\nEpoch 89/200\n132/132 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9257\nEpoch 00089: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 130s 987ms/step - loss: 0.2805 - accuracy: 0.9257 - val_loss: 0.5810 - val_accuracy: 0.7692\nEpoch 90/200\n132/132 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9152\nEpoch 00090: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 131s 991ms/step - loss: 0.2947 - accuracy: 0.9152 - val_loss: 0.4232 - val_accuracy: 0.8490\nEpoch 91/200\n132/132 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9195\nEpoch 00091: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 132s 998ms/step - loss: 0.2915 - accuracy: 0.9195 - val_loss: 0.3939 - val_accuracy: 0.8768\nEpoch 92/200\n132/132 [==============================] - ETA: 0s - loss: 0.2870 - accuracy: 0.9221\nEpoch 00092: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 131s 995ms/step - loss: 0.2870 - accuracy: 0.9221 - val_loss: 0.3826 - val_accuracy: 0.8718\nEpoch 93/200\n132/132 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.9223\nEpoch 00093: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 135s 1s/step - loss: 0.2865 - accuracy: 0.9223 - val_loss: 0.5211 - val_accuracy: 0.8262\nEpoch 94/200\n132/132 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9278\nEpoch 00094: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 138s 1s/step - loss: 0.2677 - accuracy: 0.9278 - val_loss: 0.9701 - val_accuracy: 0.7372\nEpoch 95/200\n132/132 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9257\nEpoch 00095: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.2868 - accuracy: 0.9257 - val_loss: 0.4096 - val_accuracy: 0.8547\nEpoch 96/200\n132/132 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9238\nEpoch 00096: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 138s 1s/step - loss: 0.2822 - accuracy: 0.9238 - val_loss: 0.4435 - val_accuracy: 0.8561\nEpoch 97/200\n132/132 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9297\nEpoch 00097: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 138s 1s/step - loss: 0.2804 - accuracy: 0.9297 - val_loss: 0.5873 - val_accuracy: 0.8041\nEpoch 98/200\n132/132 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9216\nEpoch 00098: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 134s 1s/step - loss: 0.2919 - accuracy: 0.9216 - val_loss: 0.3770 - val_accuracy: 0.8697\nEpoch 99/200\n132/132 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.9276\nEpoch 00099: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 133s 1s/step - loss: 0.2689 - accuracy: 0.9276 - val_loss: 0.6294 - val_accuracy: 0.8170\nEpoch 100/200\n132/132 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9254\nEpoch 00100: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 137s 1s/step - loss: 0.2739 - accuracy: 0.9254 - val_loss: 0.5163 - val_accuracy: 0.8526\nEpoch 101/200\n132/132 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.9216\nEpoch 00101: val_accuracy did not improve from 0.88818\n132/132 [==============================] - 134s 1s/step - loss: 0.2821 - accuracy: 0.9216 - val_loss: 0.5622 - val_accuracy: 0.8184\nEpoch 102/200\n132/132 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9325\nEpoch 00102: val_accuracy improved from 0.88818 to 0.89316, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_102.h5\n132/132 [==============================] - 133s 1s/step - loss: 0.2590 - accuracy: 0.9325 - val_loss: 0.3609 - val_accuracy: 0.8932\nEpoch 103/200\n132/132 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9387\nEpoch 00103: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 138s 1s/step - loss: 0.2464 - accuracy: 0.9387 - val_loss: 0.3844 - val_accuracy: 0.8775\nEpoch 104/200\n132/132 [==============================] - ETA: 0s - loss: 0.2718 - accuracy: 0.9283\nEpoch 00104: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 133s 1s/step - loss: 0.2718 - accuracy: 0.9283 - val_loss: 0.4213 - val_accuracy: 0.8469\nEpoch 105/200\n132/132 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9264\nEpoch 00105: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 132s 1s/step - loss: 0.2723 - accuracy: 0.9264 - val_loss: 0.4654 - val_accuracy: 0.8647\nEpoch 106/200\n132/132 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9304\nEpoch 00106: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 136s 1s/step - loss: 0.2629 - accuracy: 0.9304 - val_loss: 0.4950 - val_accuracy: 0.8668\nEpoch 107/200\n132/132 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9340\nEpoch 00107: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 135s 1s/step - loss: 0.2574 - accuracy: 0.9340 - val_loss: 0.5022 - val_accuracy: 0.8426\nEpoch 108/200\n132/132 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9382\nEpoch 00108: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 136s 1s/step - loss: 0.2560 - accuracy: 0.9382 - val_loss: 0.4284 - val_accuracy: 0.8697\nEpoch 109/200\n132/132 [==============================] - ETA: 0s - loss: 0.2616 - accuracy: 0.9295\nEpoch 00109: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 135s 1s/step - loss: 0.2616 - accuracy: 0.9295 - val_loss: 0.4217 - val_accuracy: 0.8789\nEpoch 110/200\n132/132 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9285\nEpoch 00110: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 135s 1s/step - loss: 0.2682 - accuracy: 0.9285 - val_loss: 0.4233 - val_accuracy: 0.8604\nEpoch 111/200\n132/132 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9321\nEpoch 00111: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 136s 1s/step - loss: 0.2626 - accuracy: 0.9321 - val_loss: 0.5920 - val_accuracy: 0.8590\n","name":"stdout"},{"output_type":"stream","text":"Epoch 112/200\n132/132 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9380\nEpoch 00112: val_accuracy did not improve from 0.89316\n132/132 [==============================] - 135s 1s/step - loss: 0.2509 - accuracy: 0.9380 - val_loss: 0.7835 - val_accuracy: 0.7813\nEpoch 113/200\n132/132 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9309\nEpoch 00113: val_accuracy improved from 0.89316 to 0.89530, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_113.h5\n132/132 [==============================] - 137s 1s/step - loss: 0.2653 - accuracy: 0.9309 - val_loss: 0.3551 - val_accuracy: 0.8953\nEpoch 114/200\n132/132 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9330\nEpoch 00114: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2511 - accuracy: 0.9330 - val_loss: 0.3643 - val_accuracy: 0.8917\nEpoch 115/200\n132/132 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9361\nEpoch 00115: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 136s 1s/step - loss: 0.2384 - accuracy: 0.9361 - val_loss: 0.4365 - val_accuracy: 0.8689\nEpoch 116/200\n132/132 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9423\nEpoch 00116: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2306 - accuracy: 0.9423 - val_loss: 0.4721 - val_accuracy: 0.8504\nEpoch 117/200\n132/132 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9333\nEpoch 00117: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2585 - accuracy: 0.9333 - val_loss: 0.5053 - val_accuracy: 0.8568\nEpoch 118/200\n132/132 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.9328\nEpoch 00118: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2650 - accuracy: 0.9328 - val_loss: 0.4726 - val_accuracy: 0.8625\nEpoch 119/200\n132/132 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9373\nEpoch 00119: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 136s 1s/step - loss: 0.2537 - accuracy: 0.9373 - val_loss: 0.4347 - val_accuracy: 0.8704\nEpoch 120/200\n132/132 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9352\nEpoch 00120: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2407 - accuracy: 0.9352 - val_loss: 0.5123 - val_accuracy: 0.8426\nEpoch 121/200\n132/132 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9361\nEpoch 00121: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 136s 1s/step - loss: 0.2543 - accuracy: 0.9361 - val_loss: 0.3760 - val_accuracy: 0.8825\nEpoch 122/200\n132/132 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9392\nEpoch 00122: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2437 - accuracy: 0.9392 - val_loss: 0.4480 - val_accuracy: 0.8925\nEpoch 123/200\n132/132 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9444\nEpoch 00123: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 137s 1s/step - loss: 0.2321 - accuracy: 0.9444 - val_loss: 0.4611 - val_accuracy: 0.8590\nEpoch 124/200\n132/132 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9401\nEpoch 00124: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 135s 1s/step - loss: 0.2568 - accuracy: 0.9401 - val_loss: 0.4075 - val_accuracy: 0.8604\nEpoch 125/200\n132/132 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9378\nEpoch 00125: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 136s 1s/step - loss: 0.2490 - accuracy: 0.9378 - val_loss: 0.5310 - val_accuracy: 0.8426\nEpoch 126/200\n132/132 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9347\nEpoch 00126: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 136s 1s/step - loss: 0.2478 - accuracy: 0.9347 - val_loss: 0.3368 - val_accuracy: 0.8953\nEpoch 127/200\n132/132 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9416\nEpoch 00127: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 139s 1s/step - loss: 0.2332 - accuracy: 0.9416 - val_loss: 0.5284 - val_accuracy: 0.8447\nEpoch 128/200\n132/132 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9444\nEpoch 00128: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 137s 1s/step - loss: 0.2344 - accuracy: 0.9444 - val_loss: 0.5783 - val_accuracy: 0.8526\nEpoch 129/200\n132/132 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.9447\nEpoch 00129: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 139s 1s/step - loss: 0.2309 - accuracy: 0.9447 - val_loss: 0.4676 - val_accuracy: 0.8711\nEpoch 130/200\n132/132 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9461\nEpoch 00130: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 139s 1s/step - loss: 0.2314 - accuracy: 0.9461 - val_loss: 0.5046 - val_accuracy: 0.8654\nEpoch 131/200\n132/132 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9470\nEpoch 00131: val_accuracy did not improve from 0.89530\n132/132 [==============================] - 138s 1s/step - loss: 0.2391 - accuracy: 0.9470 - val_loss: 0.6378 - val_accuracy: 0.8504\nEpoch 132/200\n132/132 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9444\nEpoch 00132: val_accuracy improved from 0.89530 to 0.89744, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_132.h5\n132/132 [==============================] - 139s 1s/step - loss: 0.2272 - accuracy: 0.9444 - val_loss: 0.3170 - val_accuracy: 0.8974\nEpoch 133/200\n132/132 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9444\nEpoch 00133: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 139s 1s/step - loss: 0.2265 - accuracy: 0.9444 - val_loss: 0.3937 - val_accuracy: 0.8839\nEpoch 134/200\n132/132 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9439\nEpoch 00134: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 136s 1s/step - loss: 0.2263 - accuracy: 0.9439 - val_loss: 0.3731 - val_accuracy: 0.8953\nEpoch 135/200\n132/132 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9477\nEpoch 00135: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 136s 1s/step - loss: 0.2242 - accuracy: 0.9477 - val_loss: 0.5002 - val_accuracy: 0.8640\nEpoch 136/200\n132/132 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9482\nEpoch 00136: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 138s 1s/step - loss: 0.2242 - accuracy: 0.9482 - val_loss: 0.3800 - val_accuracy: 0.8967\nEpoch 137/200\n132/132 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9409\nEpoch 00137: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 139s 1s/step - loss: 0.2327 - accuracy: 0.9409 - val_loss: 0.4316 - val_accuracy: 0.8839\nEpoch 138/200\n132/132 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9527\nEpoch 00138: val_accuracy did not improve from 0.89744\n132/132 [==============================] - 137s 1s/step - loss: 0.1994 - accuracy: 0.9527 - val_loss: 0.3555 - val_accuracy: 0.8875\nEpoch 139/200\n132/132 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9513\nEpoch 00139: val_accuracy improved from 0.89744 to 0.90100, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_139.h5\n132/132 [==============================] - 140s 1s/step - loss: 0.2031 - accuracy: 0.9513 - val_loss: 0.3652 - val_accuracy: 0.9010\n","name":"stdout"},{"output_type":"stream","text":"Epoch 140/200\n132/132 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9487\nEpoch 00140: val_accuracy improved from 0.90100 to 0.90670, saving model to ./logs/_Nov22_10-44-52/checkpoint/model_cp_140.h5\n132/132 [==============================] - 141s 1s/step - loss: 0.2235 - accuracy: 0.9487 - val_loss: 0.3891 - val_accuracy: 0.9067\nEpoch 141/200\n132/132 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9461\nEpoch 00141: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 139s 1s/step - loss: 0.2231 - accuracy: 0.9461 - val_loss: 0.4367 - val_accuracy: 0.8839\nEpoch 142/200\n132/132 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9387\nEpoch 00142: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 139s 1s/step - loss: 0.2306 - accuracy: 0.9387 - val_loss: 0.4269 - val_accuracy: 0.8868\nEpoch 143/200\n132/132 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.9458\nEpoch 00143: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 140s 1s/step - loss: 0.2280 - accuracy: 0.9458 - val_loss: 0.4026 - val_accuracy: 0.8768\nEpoch 144/200\n132/132 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9482\nEpoch 00144: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 139s 1s/step - loss: 0.2183 - accuracy: 0.9482 - val_loss: 0.3449 - val_accuracy: 0.8953\nEpoch 145/200\n132/132 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9499\nEpoch 00145: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 138s 1s/step - loss: 0.2119 - accuracy: 0.9499 - val_loss: 0.4239 - val_accuracy: 0.8889\nEpoch 146/200\n132/132 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9525\nEpoch 00146: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 138s 1s/step - loss: 0.2106 - accuracy: 0.9525 - val_loss: 0.3177 - val_accuracy: 0.9003\nEpoch 147/200\n132/132 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9515\nEpoch 00147: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 138s 1s/step - loss: 0.1954 - accuracy: 0.9515 - val_loss: 0.5097 - val_accuracy: 0.8526\nEpoch 148/200\n132/132 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9515\nEpoch 00148: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 140s 1s/step - loss: 0.2172 - accuracy: 0.9515 - val_loss: 0.4643 - val_accuracy: 0.8647\nEpoch 149/200\n132/132 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9570\nEpoch 00149: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 138s 1s/step - loss: 0.1917 - accuracy: 0.9570 - val_loss: 0.7129 - val_accuracy: 0.8383\nEpoch 150/200\n132/132 [==============================] - ETA: 0s - loss: 0.2245 - accuracy: 0.9480\nEpoch 00150: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 139s 1s/step - loss: 0.2245 - accuracy: 0.9480 - val_loss: 0.4212 - val_accuracy: 0.8711\nEpoch 151/200\n132/132 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9520\nEpoch 00151: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 143s 1s/step - loss: 0.2008 - accuracy: 0.9520 - val_loss: 0.6301 - val_accuracy: 0.8333\nEpoch 152/200\n132/132 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9523\nEpoch 00152: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 142s 1s/step - loss: 0.2164 - accuracy: 0.9523 - val_loss: 0.4194 - val_accuracy: 0.8704\nEpoch 153/200\n132/132 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9570\nEpoch 00153: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 142s 1s/step - loss: 0.2051 - accuracy: 0.9570 - val_loss: 0.3400 - val_accuracy: 0.8989\nEpoch 154/200\n132/132 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9485\nEpoch 00154: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 140s 1s/step - loss: 0.2061 - accuracy: 0.9485 - val_loss: 0.5392 - val_accuracy: 0.8526\nEpoch 155/200\n132/132 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9520\nEpoch 00155: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 140s 1s/step - loss: 0.2182 - accuracy: 0.9520 - val_loss: 0.3785 - val_accuracy: 0.8889\nEpoch 156/200\n132/132 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9539\nEpoch 00156: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 134s 1s/step - loss: 0.1873 - accuracy: 0.9539 - val_loss: 0.6364 - val_accuracy: 0.8319\nEpoch 157/200\n132/132 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9501\nEpoch 00157: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 136s 1s/step - loss: 0.2059 - accuracy: 0.9501 - val_loss: 0.4393 - val_accuracy: 0.8846\nEpoch 158/200\n132/132 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9596\nEpoch 00158: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 135s 1s/step - loss: 0.1847 - accuracy: 0.9596 - val_loss: 0.3887 - val_accuracy: 0.8825\nEpoch 159/200\n132/132 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9537\nEpoch 00159: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1000ms/step - loss: 0.1887 - accuracy: 0.9537 - val_loss: 0.4533 - val_accuracy: 0.8825\nEpoch 160/200\n132/132 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9542\nEpoch 00160: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1s/step - loss: 0.2000 - accuracy: 0.9542 - val_loss: 0.6825 - val_accuracy: 0.8397\nEpoch 161/200\n132/132 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9606\nEpoch 00161: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1s/step - loss: 0.1723 - accuracy: 0.9606 - val_loss: 0.3959 - val_accuracy: 0.8996\nEpoch 162/200\n132/132 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9525\nEpoch 00162: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 133s 1s/step - loss: 0.2063 - accuracy: 0.9525 - val_loss: 0.4206 - val_accuracy: 0.8932\nEpoch 163/200\n132/132 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.9610\nEpoch 00163: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 999ms/step - loss: 0.1960 - accuracy: 0.9610 - val_loss: 0.3893 - val_accuracy: 0.8896\nEpoch 164/200\n132/132 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9572\nEpoch 00164: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1s/step - loss: 0.1843 - accuracy: 0.9572 - val_loss: 0.4700 - val_accuracy: 0.8754\nEpoch 165/200\n132/132 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9542\nEpoch 00165: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 133s 1s/step - loss: 0.1994 - accuracy: 0.9542 - val_loss: 0.4138 - val_accuracy: 0.8825\nEpoch 166/200\n132/132 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9608\nEpoch 00166: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 133s 1s/step - loss: 0.1872 - accuracy: 0.9608 - val_loss: 0.3789 - val_accuracy: 0.8882\nEpoch 167/200\n132/132 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9584\nEpoch 00167: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 133s 1s/step - loss: 0.1875 - accuracy: 0.9584 - val_loss: 0.4456 - val_accuracy: 0.8789\nEpoch 168/200\n132/132 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9641\nEpoch 00168: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 133s 1s/step - loss: 0.1556 - accuracy: 0.9641 - val_loss: 0.3851 - val_accuracy: 0.8974\n","name":"stdout"},{"output_type":"stream","text":"Epoch 169/200\n132/132 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9591\nEpoch 00169: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 996ms/step - loss: 0.1796 - accuracy: 0.9591 - val_loss: 0.3995 - val_accuracy: 0.8910\nEpoch 170/200\n132/132 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9608\nEpoch 00170: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1s/step - loss: 0.1852 - accuracy: 0.9608 - val_loss: 0.4073 - val_accuracy: 0.8675\nEpoch 171/200\n132/132 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9572\nEpoch 00171: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 134s 1s/step - loss: 0.1858 - accuracy: 0.9572 - val_loss: 0.4073 - val_accuracy: 0.8903\nEpoch 172/200\n132/132 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9594\nEpoch 00172: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 1000ms/step - loss: 0.1770 - accuracy: 0.9594 - val_loss: 0.4038 - val_accuracy: 0.8739\nEpoch 173/200\n132/132 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.9632\nEpoch 00173: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 132s 996ms/step - loss: 0.1753 - accuracy: 0.9632 - val_loss: 0.6364 - val_accuracy: 0.8226\nEpoch 174/200\n132/132 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9601\nEpoch 00174: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 989ms/step - loss: 0.1870 - accuracy: 0.9601 - val_loss: 0.3267 - val_accuracy: 0.9017\nEpoch 175/200\n132/132 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9570\nEpoch 00175: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 995ms/step - loss: 0.2016 - accuracy: 0.9570 - val_loss: 0.3761 - val_accuracy: 0.8896\nEpoch 176/200\n132/132 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.9675\nEpoch 00176: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 993ms/step - loss: 0.1588 - accuracy: 0.9675 - val_loss: 0.4572 - val_accuracy: 0.8604\nEpoch 177/200\n132/132 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9603\nEpoch 00177: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 990ms/step - loss: 0.1736 - accuracy: 0.9603 - val_loss: 0.4168 - val_accuracy: 0.8868\nEpoch 178/200\n132/132 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9620\nEpoch 00178: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 991ms/step - loss: 0.1854 - accuracy: 0.9620 - val_loss: 0.5507 - val_accuracy: 0.8519\nEpoch 179/200\n132/132 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9629\nEpoch 00179: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 991ms/step - loss: 0.1817 - accuracy: 0.9629 - val_loss: 0.4077 - val_accuracy: 0.8889\nEpoch 180/200\n132/132 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9615\nEpoch 00180: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 987ms/step - loss: 0.1621 - accuracy: 0.9615 - val_loss: 0.5215 - val_accuracy: 0.8718\nEpoch 181/200\n132/132 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9613\nEpoch 00181: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 988ms/step - loss: 0.1750 - accuracy: 0.9613 - val_loss: 0.3856 - val_accuracy: 0.8946\nEpoch 182/200\n132/132 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9637\nEpoch 00182: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 129s 980ms/step - loss: 0.1754 - accuracy: 0.9637 - val_loss: 0.3626 - val_accuracy: 0.9046\nEpoch 183/200\n132/132 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9646\nEpoch 00183: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 129s 981ms/step - loss: 0.1764 - accuracy: 0.9646 - val_loss: 0.5127 - val_accuracy: 0.8782\nEpoch 184/200\n132/132 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9637\nEpoch 00184: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 987ms/step - loss: 0.1808 - accuracy: 0.9637 - val_loss: 0.4819 - val_accuracy: 0.8697\nEpoch 185/200\n132/132 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9675\nEpoch 00185: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 987ms/step - loss: 0.1608 - accuracy: 0.9675 - val_loss: 0.3765 - val_accuracy: 0.8839\nEpoch 186/200\n132/132 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9637\nEpoch 00186: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 994ms/step - loss: 0.1672 - accuracy: 0.9637 - val_loss: 0.5193 - val_accuracy: 0.8682\nEpoch 187/200\n132/132 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9641\nEpoch 00187: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 990ms/step - loss: 0.1676 - accuracy: 0.9641 - val_loss: 0.5397 - val_accuracy: 0.8625\nEpoch 188/200\n132/132 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9610\nEpoch 00188: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 131s 994ms/step - loss: 0.1745 - accuracy: 0.9610 - val_loss: 0.7792 - val_accuracy: 0.8326\nEpoch 189/200\n132/132 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9651\nEpoch 00189: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 985ms/step - loss: 0.1789 - accuracy: 0.9651 - val_loss: 0.3659 - val_accuracy: 0.8981\nEpoch 190/200\n132/132 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9727\nEpoch 00190: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 129s 981ms/step - loss: 0.1447 - accuracy: 0.9727 - val_loss: 0.4004 - val_accuracy: 0.8832\nEpoch 191/200\n132/132 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9622\nEpoch 00191: val_accuracy did not improve from 0.90670\n132/132 [==============================] - 130s 986ms/step - loss: 0.1780 - accuracy: 0.9622 - val_loss: 0.3709 - val_accuracy: 0.8953\nEpoch 192/200\n 72/132 [===============>..............] - ETA: 53s - loss: 0.1743 - accuracy: 0.9635","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--------------Saving the model---------------\n#---------------------------------------------\n\n\nfrom datetime import datetime\n\nsavedir = os.path.join('./savedModel'+ datetime.now().strftime('%b%d_%H-%M-%S'))\n\nif not os.path.exists(savedir):\n    os.makedirs(savedir) \n\nmodel.save(savedir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clmode = \"rgb\"\nsource = '../input/artificial-neural-networks-and-deep-learning-2020/MaskDataset'\n\ntest_data_gen = ImageDataGenerator(rescale = 1./255)\n\ntest_datagen = test_data_gen.flow_from_directory(\n    source,\n    target_size = (256, 256),\n    color_mode = clmode,\n    classes =  [\"test\"],\n    class_mode = \"categorical\",\n    batch_size = 1,\n    shuffle = False\n)\n\ntest_datagen.reset()","execution_count":6,"outputs":[{"output_type":"stream","text":"Found 450 images belonging to 1 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_generator(test_datagen, len(test_datagen), verbose = 1)\nresult = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ntpath\n\nimages = test_datagen.filenames\ni = 0\n\nfor p in predictions:\n    prediction = np.argmax(p)\n    image_name = ntpath.basename(images[i])\n    result[image_name] = str(prediction)\n    i = i+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_csv(results):\n\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(os.path.join('./', csv_fname), 'w') as f:\n\n        f.write('Id,Category\\n')\n\n        for key, value in results.items():\n            f.write(key + ',' + str(value) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_csv(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}