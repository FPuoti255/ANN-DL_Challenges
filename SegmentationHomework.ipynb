{"cells":[{"source":["# Artifical Neural Networks & Deep Learning\n","# Homework 2 - Image Segmentation\n","# Selected subset : Bipbip - Haricot\n","\n","**Developement Team:**\n","- Acquati Marco - 10583134 \n","- Brugali Giorgio - 10794550\n","- Puoti Francesco - 10595640 \n","\n","\n","# *1. Data acquisition and augmentation*\n","> We decided to tile images in 256 x 256 sub_images. Thus, for each image in the dataSet, we extracted 48 sub_images.\n","Then, we decided to manage by ourselfs data augmentation:\n","- Each image of the dataset is included resized;\n","- For each tile previously extracted, it's applied either random-angle rotation or vertical flip or horizontal flip. Obviously, for the tiles who are composed by only black pixels, the data augmentation is not applied, so that we do not run out of ram.\n","\n","> Further information about this topic has been issued with annotations as the code flows down, to better clarify the correspondence between the explanations and the code snippets \n","\n","> **1.1. MaskCleaner**\n",">> It's noteworthy the mask acquisition. In fact, we used the maskCleaner function. Masks are loaded in grey scale mode, then noises are deleted with the aforementioned function. The resulting masks are composed by pixels with values in {0, 1, 2} (one value for each class).\n","\n","\n","\n","# *2. Model overview*\n","\n","\n","> **2.1. The Network**\n",">> We based our model on the U-Net. It's composed by 7 convBlock. Each of the latters consists of two convolutional layer. The filters number starts from 64 and it's subsequently duplicated until 512, that is the bottleneck of our net. We tried also with 1024 but we figured out that the 512 U-net is less prone to noise effects.\n","Regarding the upsampling, we opted for a transpose convolutional instead of normal UpSampling, to leverage the fact that the Transpose Convolutional layer learns how to fill in details during the model training process.\n","\n","> **2.2. Classification and Prediction process**\n",">> As output layer, we used a Convolutional one that classifies by means of the softmax activation function. since we crop the images before of feeding them into the network, after the prediction, we reconstruct the mask, taking 48 subimages at once.\n","\n","> **2.3. Optimizer & LossFunction** \n",">> - Adam, with a starting learning rate of 1e-3 and amsgrad = True to have an adaptive learning rate, so as to prevent the network from being stuck on a suboptimal solution.\n",">> - Loss function : Sparse Categorical Crossentropy.\n","\n","> **2.4. Metrics** \n",">> As metric, we chose the MeanIoU of Keras, but we gave a weight for each class:\n","- weight = 1 for both crop and weed;\n","- weight = 0.3 for the background.\n","\n","> **2.5. Further information about the implemention process**\n",">> No EarlyStopping has been used in the final model as, after some trials, such model got stopped even though the learning process would have subsequently led to noteworthy improvements.\n","\n",">> With regard to the checkpoints, we initially implemented them but, the more the network was becoming complex, the bigger was the space occupied on the HDD.\n","Therefore, we need to avoid to use checkpoint, otherwise HDD space on kaggle as well as colab would have ran out of memory.\n","\n",">> Number of epochs is set to 75, since we noticed that the more the networks trains after 7, the more it produces only distorted images\n","\n",">>In order to preserve some RAM memory space we explicitly called the garbage collector and the del keyword for deleting the no more used images arrays."],"cell_type":"markdown","metadata":{}},{"metadata":{"id":"MZ_sx9v8pXNr","trusted":true},"cell_type":"code","source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import os\n","import gc\n","import tensorflow as tf\n","import numpy as np\n","\n","# Set the seed for random operations. \n","# This let our experiments to be reproducible. \n","SEED = 1234\n","tf.random.set_seed(SEED)  \n","np.random.seed(SEED)\n","cwd = os.getcwd()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"cFBY4HKOpXNz","trusted":true},"cell_type":"code","source":["import PIL\n","\n","def cropImg(img): #Function used to crop the images. \n","    #print(img.shape)\n","    img = np.expand_dims(img, axis= 0)\n","   # img = np.expand_dims(img, axis = -1)\n","    crop = tf.image.extract_patches(images= img,\n","                             sizes=[1, 256,256, 1],\n","                            strides=[1, 256,256, 1],\n","                            rates=[1, 1, 1, 1],\n","                            padding='SAME')\n","    crop = np.array(crop)\n","    #print(crop.shape)\n","    crop = crop.reshape(48*len(img),256,256,3)\n","    return crop\n","\n","def cropMask(img): #Function used to crop the masks. \n","    img = np.expand_dims(img, axis= 0)\n","    img = np.expand_dims(img, axis = -1)\n","    crop = tf.image.extract_patches(images= img,\n","                             sizes=[len(img), 256,256, 1],\n","                            strides=[len(img), 256,256, 1],\n","                            rates=[1, 1, 1, 1],\n","                            padding='SAME')\n","    crop = np.array(crop)\n","    crop = crop.reshape(48*len(img),256,256,1)\n","    return crop\n","\n","def maskCleaner(img): #Function used to convert the masks' pixel in our class values.\n","    mask = np.ones_like(img) *2 #weed\n","    mask[np.where(img == 0)] = 0 #background\n","    mask[np.where(img == 255)] = 1 #haricot\n","    return mask\n","\n","\n","#GET IMAGES:\n","#This function load all the images from the specified path and calls the functions above. \n","#It returns two array, one with the cropped images and the other with the cropped masks.\n","def loadImages(idx):\n","    images = []\n","    masks = []\n","    paths =[ '../input/development-datasetzip/Development_Dataset/Training/Bipbip/Haricot'] \n","    for dirs,_,files in os.walk(str(paths[idx])  +'/Images'):\n","        files.sort()\n","        for f in files:\n","            img = PIL.Image.open(os.path.join(paths[idx] + '/Images',f))\n","            img = np.array(img)\n","            img = cropImg(img)\n","            images.append(img)\n","    for dirs,_,files in os.walk(paths[idx] + '/Masks'):        \n","        files.sort()\n","        for f in files:\n","            mask = PIL.Image.open(os.path.join(paths[idx] + '/Masks',f)).convert('I')\n","            mask = np.array(mask)\n","            mask = maskCleaner(mask)\n","            mask= cropMask(mask)\n","            masks.append(mask)\n","    return images, masks\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["images , masks = loadImages(0)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6LSFocx7pXN0","outputId":"e50c55fb-a2ee-4f13-a53f-d6537a43a7e6","trusted":true},"cell_type":"code","source":["#Arrays reshape from (90,48,256,356,channels) to (90*48,256,256,channels) \n","images = np.array(images)\n","images = images.reshape([90*48,256,256,3])\n","masks = np.array(masks)\n","masks = masks.reshape([90*48,256,256,1])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Data set split in training and validation sets\n","# with the latter having a size equal to the 20% of the entire data set.\n","#-----------------------------------------------------------------------\n","images_train, images_valid, masks_train, masks_valid = train_test_split(images, masks, test_size=0.2, shuffle = True, random_state = SEED)\n","\n","\n","del images\n","del masks\n","\n","gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#These two functions are used to perform DataAugmentation on both images and masks.\n","\n","from scipy import ndimage\n","def DataAugmetation(img_, mask_):\n","  \n","  aug_imgs = [img_]\n","  aug_masks = [mask_]\n","  if np.all(np.unique(mask_) == 0): #if the mask is composed by black pixels only, it's useless to perform data augmentation\n","    return aug_imgs, aug_masks\n","\n","  operation = np.random.randint(low = 0, high=3) #low (inclusive) to high (exclusive)\n","  if operation == 0:\n","      #rotation\n","      angle = int(np.random.uniform(0,360,1))\n","      aug_imgs.append(ndimage.rotate(img_, angle, reshape=False, order=0, mode = 'nearest'))\n","      aug_masks.append(ndimage.rotate(mask_, angle, reshape=False, order=0, mode = 'nearest'))\n","  elif operation == 1:\n","      #vertical flip\n","      aug_imgs.append(np.flipud(img_))\n","      aug_masks.append(np.flipud(mask_))\n","  else:\n","      #horizontal flip\n","      aug_imgs.append(np.fliplr(img_))\n","      aug_masks.append(np.fliplr(mask_))\n","  return aug_imgs, aug_masks\n","\n","\n","def dataAugment():\n","  augImages = []\n","  augMasks = []\n","  for i in range(len(images_train)):\n","    x,y =DataAugmetation(images_train[i],masks_train[i])\n","    for k in x:\n","      augImages.append(k)\n","    for j in y:\n","      augMasks.append(j)\n","  return augImages, augMasks\n","\n","\n","augImages, augMasks = dataAugment()\n","augImages = np.array(augImages)\n","augMasks = np.array(augMasks)\n","\n","\n","del images_train\n","del masks_train\n","\n","gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator             \n","\n","train_data_gen = ImageDataGenerator(rescale = 1./255)   \n","valid_data_gen = ImageDataGenerator(rescale = 1./255)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"cx8xNUSMpXN7","trusted":true},"cell_type":"code","source":["\n","# Batch size\n","bs = 32\n","\n","# img shape\n","img_h = 256\n","img_w = 256\n","\n","train_gen = train_data_gen.flow(tf.convert_to_tensor(augImages),\n","                                augMasks,\n","                                batch_size=bs,\n","                                shuffle=True,\n","                                seed=SEED\n","                                ) \n","del augImages\n","del augMasks\n","\n","valid_gen = valid_data_gen.flow(tf.convert_to_tensor(images_valid),\n","                                masks_valid,\n","                                batch_size=bs,\n","                                shuffle=True,\n","                                seed=SEED\n","                                ) \n","\n","del images_valid\n","del masks_valid\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"1LNwPKFvpXN8","trusted":true},"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n","valid_dataset = valid_dataset.repeat()\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9I82CDWCuLkM","outputId":"bc70b8f4-8ddf-49a4-bf7e-826d3afd71c3","trusted":true},"cell_type":"code","source":["gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"XP2e20dSL1JW","trusted":true},"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","inp_shape = (img_h, img_w, 3)\n","num_classes = 3\n","initializer = tf.keras.initializers.HeNormal()\n","\n","def conv2D_block(numFilt_, previouslayer_) :\n","\n","    convblock = layers.Conv2D(filters=numFilt_, kernel_size = (3,3), strides=1, padding=\"same\", kernel_initializer=initializer)(previouslayer_)\n","    convblock = layers.BatchNormalization()(convblock)\n","    convblock = layers.ReLU()(convblock)\n","    convblock = layers.Conv2D(filters=numFilt_, kernel_size = (3,3), strides=1, padding=\"same\", kernel_initializer=initializer)(convblock)\n","    convblock = layers.BatchNormalization()(convblock)\n","    convblock = layers.ReLU()(convblock)\n","\n","    return convblock\n","\n","\n","def create_UNet(num_classes):    \n","\n","    \n","#-----------------ENCODER PART--------------------------------\n","#-------------------------------------------------------------\n","    inputs = tf.keras.Input(shape=inp_shape)\n","\n","    conv1 = conv2D_block(64, inputs)\n","    pool1 = layers.MaxPool2D(pool_size=(2,2), strides = 2)(conv1)\n","    \n","    conv2 = conv2D_block(128, pool1)\n","    pool2 = layers.MaxPool2D(pool_size=(2,2), strides = 2)(conv2)\n","\n","    conv3 = conv2D_block(256 , pool2)\n","    pool3 = layers.MaxPool2D(pool_size=(2,2), strides = 2)(conv3)\n","\n","\n","#-----------------MIDDLE PART -- BOTTOM OF THE U-NET-------------------\n","#----------------------------------------------------------------------\n","\n","    btn = conv2D_block(512 , pool3)\n","    \n","#----------------------DECODER PART----------------------------\n","#--------------------------------------------------------------\n"," \n","    up1 = layers.Conv2DTranspose(filters=256, kernel_size = (2,2), strides = 2)(btn)\n","    conv5 = layers.Add()([up1, conv3])\n","    conv5 = conv2D_block(256 ,conv5)\n","\n","    up2 = layers.Conv2DTranspose(filters=128, kernel_size = (2,2), strides = 2)(conv5)\n","    conv6 = layers.Add()([up2, conv2])\n","    conv6 = conv2D_block(128 , conv6)\n","\n","    up3 = layers.Conv2DTranspose(filters=64, kernel_size = (2,2), strides = 2)(conv6)\n","    conv7 = layers.Add()([up3, conv1])\n","    conv7 = conv2D_block(64 , conv7)\n","\n","    outputs = layers.Conv2D(filters=num_classes,\n","                            kernel_size=(1, 1),\n","                            strides=(1, 1),\n","                            padding='same',\n","                            activation='softmax',\n","                            kernel_initializer = initializer)(conv7)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, outputs)\n","    return model \n","  "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["model = create_UNet(num_classes)\n","model.summary()\n","gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"fmxDUgPIL8iL","trusted":true},"cell_type":"code","source":["# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","\n","lr = 1e-3   # learning rate\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr, amsgrad=True)\n","\n","iou = tf.keras.metrics.MeanIoU(num_classes=3)\n","def IoU(y_true, y_pred): #MeanIoU function which uses keras MeanIoU\n","    y_pred = tf.expand_dims(tf.cast(tf.argmax(y_pred, -1), tf.float32), -1)\n","    y_true = tf.cast(y_true, tf.float32)\n","    weights = tf.where(tf.math.logical_or(y_true == 1.0 , y_true == 2.0), 1.0, 0.3)\n","    iou.update_state(y_true, y_pred, sample_weight = weights)\n","    return iou.result()\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', IoU]\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6XVsdDOvXY2x","outputId":"cec02a4a-84ce-4bec-a71f-74c56b3d031d","trusted":true},"cell_type":"code","source":["gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ytKCQOJwMHAW","outputId":"6e44fdaf-fad6-4a17-eef1-a45feec785f4","trusted":true},"cell_type":"code","source":["num_ep = 75 #Training Epochs\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","model.fit(x=train_dataset,\n","          epochs=num_ep,\n","          steps_per_epoch=(len(train_gen)),\n","          validation_data=valid_dataset,\n","          validation_steps=(len(valid_gen))\n","         )\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"3I0CN8AgOGop","trusted":true},"cell_type":"code","source":["#Model saving.\n","from datetime import datetime\n","md_dir = './'\n","if not os.path.exists(md_dir):\n","    os.makedirs(md_dir)\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","model_path = os.path.join(md_dir,'model_' + str(now) + '.h5')\n","model.save(model_path)\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"8AFqubOvAmKa","trusted":true},"cell_type":"code","source":["#Output prediction of the model.\n","import matplotlib.pyplot as plt\n","import PIL\n","\n","def reconstruct(patches, isMask = False): #Function used to reconstruct the predicted mask from the 48 predictions given.\n","  rows = tf.split(patches,1536//256,axis=0)\n","  rows = [tf.concat(tf.unstack(x),axis=1) for x in rows] \n","  reconstructed = tf.concat(rows,axis=0)\n","  if not isMask:\n","    return reconstructed\n","  else:\n","    return np.squeeze(reconstructed)\n","\n","# YOU HAVE THESE FUNCTIONS DEFINED ABOVE\n","#  -- def cropImg(img) -- def cropMask(img) -- def maskCleaner(img)\n","    \n","\n","image = PIL.Image.open('../input/development-datasetzip/Development_Dataset/Training/Bipbip/Haricot/Images/Bipbip_haricot_im_05231.jpg')\n","mask = PIL.Image.open('../input/development-datasetzip/Development_Dataset/Training/Bipbip/Haricot/Masks/Bipbip_haricot_im_05231.png').convert('I')\n","image = np.array(image)\n","mask = np.array(mask)\n","\n","images = cropImg(image)\n","predict = model.predict(x= images)\n","predict = np.array(tf.argmax(predict, axis = -1))\n","predict = reconstruct(predict, True)\n","\n","prediction_img = np.zeros_like([predict])\n","prediction_img = prediction_img[0]\n","\n","for i in range(1,3):\n","  prediction_img[np.where(predict == i)] = [i]\n","\n","fig, ax = plt.subplots(1,3, figsize=(20, 20))\n","print(np.unique(predict))\n","ax[0].imshow(image)\n","ax[1].imshow(predict)\n","ax[2].imshow(maskCleaner(mask))"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.0-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}